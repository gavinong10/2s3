#!/usr/bin/env python

# Perform multipart upload to Amazon S3 of data read from stdin.
#
# Example usage:
#     tar -C / -cpjO /home | 2s3 -k aws.key -b com-example-backup -o home.tar.bz2

import boto
import sys
import time
import traceback
import hashlib
import math
from cStringIO import StringIO
import click
from humanfriendly import parse_size

def upload(bucket, s3object, profile, host, insecure, ordinary, size, time, debug):

    # Establish connection to S3
    try:
        if ordinary:
            calling_format = boto.config.get(
                's3', 'calling_format', 'boto.s3.connection.OrdinaryCallingFormat')
        else:
            calling_format = None
        s3_connection = boto.connect_s3(
            profile_name=profile,
            host=host,
            is_secure=not insecure,
            calling_format=calling_format)
    except:
        print "Error: Connection to S3 could not be established."
        if debug:
            print traceback.format_exc()
        sys.exit(4)

    # Check if the bucket is available
    try:
        s3_bucket     = s3_connection.lookup(bucket)
        if not s3_bucket:
            raise Exception('BucketLookupFailed')
    except:
        print "Error: Bucket %s is not available." % bucket
        if debug:
            print traceback.format_exc()
        sys.exit(5)

    if s3_bucket.get_key(s3object):
        print "Error: Object %s exists in bucket %s." % (s3object, bucket)
        sys.exit(6)

    # Initiate the upload
    try:
        s3_upload     = s3_bucket.initiate_multipart_upload(s3object)
    except:
        print "Error: Multipart upload could not be initiated."
        if debug:
            print traceback.format_exc()
        sys.exit(7)

    # Read size bytes from stdin and upload it as a multipart to S3.
    # The md5sum of each part is calculated, and the md5sum of the concatinated
    # checksums of each part is calculated on the way to verify the files
    # integrity after upload by comparing calculated checksum with
    # eTag of uploaded object.

    uploadPart = 0
    uploadHash = hashlib.md5()

    printByteLength = str(math.ceil(math.log10(size)))
    while True:
        bytes = sys.stdin.read(size)
        if not bytes:
            print "Reached end of inputstream."
            break

        uploadPart += 1
        uploadPartHash = hashlib.md5(bytes)
        uploadHash.update(uploadPartHash.digest())

        # If upload fails, try again.
        uploadPartTry = 0
        while True:
            try:
                uploadPartTry+=1
                print ("Upload part %010d - %"+printByteLength+"d Bytes - %s - Try %d") % (uploadPart,len(bytes),uploadPartHash.hexdigest(),uploadPartTry)
                s3_upload.upload_part_from_file(StringIO(bytes),uploadPart)
                break
            except:
                print "Error uploading part. Try again in %d seconds..." % time
                if debug:
                    print traceback.format_exc()
                time.sleep(time)


    # Complete upload and check integrity
    try:
        s3_upload.complete_upload()

        # Compare the eTag of the uploaded object with the localy calculated md5sum.
        checksum_remote = s3_bucket.get_key(s3object).etag.strip('"')
        checksum_local  = uploadHash.hexdigest()+"-"+str(uploadPart)
        if checksum_remote <> checksum_local:
            print "Error: Local checksum differs from object's eTag:"
            print "Local : %s" % checksum_local
            print "Remote: %s" % checksum_remote
            print "The upload might be corrupt."
            raise Exception('ChecksumMismatch')

        print "Upload completed"
    except:
        print "Error: Error while completing upload."
        if debug:
            print traceback.format_exc()
        sys.exit(8)


CONTEXT_SETTINGS = dict(help_option_names=['-h', '--help'])

@click.command(context_settings=CONTEXT_SETTINGS)
@click.argument("bucket")#, help="Name of the target bucket.")
@click.argument("s3object", metavar='OBJECT')#, help="Name of the target object.")
@click.option("-p", "--profile", metavar='PROFILE', default=None, help="Amazon profile (configure in ~/.aws/...)")
@click.option("-H", "--host", metavar='HOST', default=None, help="Hostname or IP of the AWS endpoint")
@click.option("-i", "--insecure", is_flag=True, help="Insecure HTTP connection")
@click.option("-o", "--ordinary", is_flag=True, help="Use ordinary calling format instead of subdomain calling format")
@click.option("-s", "--size", metavar='SIZE', default='5MB', show_default=True, help="Split upload in CHUNK_SIZE")
@click.option("-t", "--time", metavar='TIME', default=5, show_default=True, help="Time in seconds to wait until retry upload a failed part again. Default is 5")
@click.option("-d", "--debug", is_flag=True, help="Print debug information")
def main(bucket, s3object, profile, host, insecure, ordinary, size, time, debug):
    """
    Send data from standard input to OBJECT in your S3 or S3-like BUCKET
    """
    upload(bucket, s3object, profile, host, insecure, ordinary, parse_size(size), time, debug)

if __name__ == '__main__':
    main()
